{
 "cells": [
  {
   "cell_type": "code",
   "id": "bea3b3f72952942d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:27:14.655510Z",
     "start_time": "2024-12-23T12:27:14.621334Z"
    }
   },
   "source": "from utils.attributes import load_data",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "482e780d79a05cde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:27:23.540483Z",
     "start_time": "2024-12-23T12:27:14.715533Z"
    }
   },
   "source": [
    "dataset, labels = load_data()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6a38fc0ab6d3b23a",
   "metadata": {},
   "source": [
    "### Wavelet transform + RNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:27:25.938203Z",
     "start_time": "2024-12-23T12:27:25.913640Z"
    }
   },
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "# Function to extract wavelet features from an audio signal\n",
    "def extract_wavelet_features(signal, wavelet='db4', level=5):\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "    features = []\n",
    "    for coeff in coeffs:\n",
    "        features.extend([np.mean(coeff), np.std(coeff)])\n",
    "    return np.array(features)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "43c50fe0423f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:27:36.275824Z",
     "start_time": "2024-12-23T12:27:26.550040Z"
    }
   },
   "source": [
    "features = []\n",
    "for data in dataset:\n",
    "    y, sr = data.audio, data.sr\n",
    "    feature_vector = extract_wavelet_features(y)\n",
    "    # Assuming data.language is already encoded as a numerical value or one-hot vector\n",
    "    combined_feature = np.hstack((feature_vector, data.language))\n",
    "    features.append(combined_feature)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3f9b8dcb4ceb151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:27:39.887499Z",
     "start_time": "2024-12-23T12:27:39.885125Z"
    }
   },
   "source": [
    "labels = np.array(labels)\n",
    "features = np.array(features)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8692c1dcbebb95df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:27:41.495558Z",
     "start_time": "2024-12-23T12:27:40.740763Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Define a custom Dataset class for PyTorch\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.as_tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "e374e046ecac9837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:27:43.105676Z",
     "start_time": "2024-12-23T12:27:43.030229Z"
    }
   },
   "source": [
    "# Create Dataset and DataLoader\n",
    "audio_dataset = AudioDataset(features, labels)\n",
    "train_size = int(0.8 * len(audio_dataset))\n",
    "test_size = len(audio_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(audio_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define the RNN model\n",
    "class AudioRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(AudioRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device)\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = features.shape[1]  # Number of input features\n",
    "hidden_size = 64\n",
    "num_layers = 5\n",
    "num_classes = len(np.unique(labels))  # Number of output classes\n",
    "num_epochs = 200\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Device configuration\n",
    "if torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2bce77256f948493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T12:28:26.288583Z",
     "start_time": "2024-12-23T12:27:44.179439Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = AudioRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for _features, _labels in train_loader_tqdm:\n",
    "        _features = _features.to(device)\n",
    "        _labels = _labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(_features.unsqueeze(1))  # Add sequence dimension\n",
    "        loss = criterion(outputs, _labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "# Evaluation with progress bar\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loader_tqdm = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _features, _labels in test_loader_tqdm:\n",
    "        _features = _features.to(device)\n",
    "        _labels = _labels.to(device)\n",
    "        outputs = model(_features.unsqueeze(1))  # Add sequence dimension\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(_labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save the model weights\n",
    "model_save_path = \"model_weights.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 35/35 [00:00<00:00, 64.71batch/s, loss=0.707]\n",
      "Epoch 2/200: 100%|██████████| 35/35 [00:00<00:00, 153.94batch/s, loss=0.698]\n",
      "Epoch 3/200: 100%|██████████| 35/35 [00:00<00:00, 163.88batch/s, loss=0.804]\n",
      "Epoch 4/200: 100%|██████████| 35/35 [00:00<00:00, 163.72batch/s, loss=0.917]\n",
      "Epoch 5/200: 100%|██████████| 35/35 [00:00<00:00, 165.40batch/s, loss=1.1]  \n",
      "Epoch 6/200: 100%|██████████| 35/35 [00:00<00:00, 165.44batch/s, loss=0.772]\n",
      "Epoch 7/200: 100%|██████████| 35/35 [00:00<00:00, 156.97batch/s, loss=0.744]\n",
      "Epoch 8/200: 100%|██████████| 35/35 [00:00<00:00, 167.71batch/s, loss=0.669]\n",
      "Epoch 9/200: 100%|██████████| 35/35 [00:00<00:00, 162.76batch/s, loss=0.705]\n",
      "Epoch 10/200: 100%|██████████| 35/35 [00:00<00:00, 164.61batch/s, loss=0.703]\n",
      "Epoch 11/200: 100%|██████████| 35/35 [00:00<00:00, 168.21batch/s, loss=0.698]\n",
      "Epoch 12/200: 100%|██████████| 35/35 [00:00<00:00, 165.60batch/s, loss=0.696]\n",
      "Epoch 13/200: 100%|██████████| 35/35 [00:00<00:00, 167.87batch/s, loss=0.829]\n",
      "Epoch 14/200: 100%|██████████| 35/35 [00:00<00:00, 163.66batch/s, loss=0.665]\n",
      "Epoch 15/200: 100%|██████████| 35/35 [00:00<00:00, 172.65batch/s, loss=0.817]\n",
      "Epoch 16/200: 100%|██████████| 35/35 [00:00<00:00, 161.55batch/s, loss=0.875]\n",
      "Epoch 17/200: 100%|██████████| 35/35 [00:00<00:00, 125.98batch/s, loss=0.688]\n",
      "Epoch 18/200: 100%|██████████| 35/35 [00:00<00:00, 170.88batch/s, loss=1.13] \n",
      "Epoch 19/200: 100%|██████████| 35/35 [00:00<00:00, 169.65batch/s, loss=0.686]\n",
      "Epoch 20/200: 100%|██████████| 35/35 [00:00<00:00, 170.67batch/s, loss=0.681]\n",
      "Epoch 21/200: 100%|██████████| 35/35 [00:00<00:00, 167.54batch/s, loss=0.782]\n",
      "Epoch 22/200: 100%|██████████| 35/35 [00:00<00:00, 165.61batch/s, loss=0.911]\n",
      "Epoch 23/200: 100%|██████████| 35/35 [00:00<00:00, 170.40batch/s, loss=0.724]\n",
      "Epoch 24/200: 100%|██████████| 35/35 [00:00<00:00, 166.40batch/s, loss=0.753]\n",
      "Epoch 25/200: 100%|██████████| 35/35 [00:00<00:00, 171.95batch/s, loss=0.711]\n",
      "Epoch 26/200: 100%|██████████| 35/35 [00:00<00:00, 172.76batch/s, loss=0.815]\n",
      "Epoch 27/200: 100%|██████████| 35/35 [00:00<00:00, 169.72batch/s, loss=0.694]\n",
      "Epoch 28/200: 100%|██████████| 35/35 [00:00<00:00, 167.81batch/s, loss=1.43] \n",
      "Epoch 29/200: 100%|██████████| 35/35 [00:00<00:00, 172.65batch/s, loss=1.12] \n",
      "Epoch 30/200: 100%|██████████| 35/35 [00:00<00:00, 164.49batch/s, loss=1.23] \n",
      "Epoch 31/200: 100%|██████████| 35/35 [00:00<00:00, 168.77batch/s, loss=0.831]\n",
      "Epoch 32/200: 100%|██████████| 35/35 [00:00<00:00, 177.78batch/s, loss=0.444]\n",
      "Epoch 33/200: 100%|██████████| 35/35 [00:00<00:00, 171.25batch/s, loss=1.27] \n",
      "Epoch 34/200: 100%|██████████| 35/35 [00:00<00:00, 173.43batch/s, loss=0.687]\n",
      "Epoch 35/200: 100%|██████████| 35/35 [00:00<00:00, 174.75batch/s, loss=0.801]\n",
      "Epoch 36/200: 100%|██████████| 35/35 [00:00<00:00, 168.48batch/s, loss=0.664]\n",
      "Epoch 37/200: 100%|██████████| 35/35 [00:00<00:00, 171.44batch/s, loss=1.09] \n",
      "Epoch 38/200: 100%|██████████| 35/35 [00:00<00:00, 171.96batch/s, loss=0.71] \n",
      "Epoch 39/200: 100%|██████████| 35/35 [00:00<00:00, 175.93batch/s, loss=0.689]\n",
      "Epoch 40/200: 100%|██████████| 35/35 [00:00<00:00, 174.68batch/s, loss=0.939]\n",
      "Epoch 41/200: 100%|██████████| 35/35 [00:00<00:00, 169.38batch/s, loss=0.83] \n",
      "Epoch 42/200: 100%|██████████| 35/35 [00:00<00:00, 172.86batch/s, loss=1.12] \n",
      "Epoch 43/200: 100%|██████████| 35/35 [00:00<00:00, 174.95batch/s, loss=1.2]  \n",
      "Epoch 44/200: 100%|██████████| 35/35 [00:00<00:00, 174.72batch/s, loss=1.13] \n",
      "Epoch 45/200: 100%|██████████| 35/35 [00:00<00:00, 172.56batch/s, loss=0.968]\n",
      "Epoch 46/200: 100%|██████████| 35/35 [00:00<00:00, 171.21batch/s, loss=0.881]\n",
      "Epoch 47/200: 100%|██████████| 35/35 [00:00<00:00, 165.40batch/s, loss=1.14] \n",
      "Epoch 48/200: 100%|██████████| 35/35 [00:00<00:00, 167.38batch/s, loss=0.806]\n",
      "Epoch 49/200: 100%|██████████| 35/35 [00:00<00:00, 170.31batch/s, loss=0.715]\n",
      "Epoch 50/200: 100%|██████████| 35/35 [00:00<00:00, 172.37batch/s, loss=0.687]\n",
      "Epoch 51/200: 100%|██████████| 35/35 [00:00<00:00, 173.53batch/s, loss=0.692]\n",
      "Epoch 52/200: 100%|██████████| 35/35 [00:00<00:00, 175.72batch/s, loss=0.692]\n",
      "Epoch 53/200: 100%|██████████| 35/35 [00:00<00:00, 175.22batch/s, loss=0.635]\n",
      "Epoch 54/200: 100%|██████████| 35/35 [00:00<00:00, 177.81batch/s, loss=0.723]\n",
      "Epoch 55/200: 100%|██████████| 35/35 [00:00<00:00, 176.97batch/s, loss=1.09] \n",
      "Epoch 56/200: 100%|██████████| 35/35 [00:00<00:00, 172.57batch/s, loss=0.747]\n",
      "Epoch 57/200: 100%|██████████| 35/35 [00:00<00:00, 170.44batch/s, loss=0.69] \n",
      "Epoch 58/200: 100%|██████████| 35/35 [00:00<00:00, 176.60batch/s, loss=0.716]\n",
      "Epoch 59/200: 100%|██████████| 35/35 [00:00<00:00, 175.47batch/s, loss=0.688]\n",
      "Epoch 60/200: 100%|██████████| 35/35 [00:00<00:00, 170.10batch/s, loss=0.739]\n",
      "Epoch 61/200: 100%|██████████| 35/35 [00:00<00:00, 175.50batch/s, loss=0.688]\n",
      "Epoch 62/200: 100%|██████████| 35/35 [00:00<00:00, 177.32batch/s, loss=0.63] \n",
      "Epoch 63/200: 100%|██████████| 35/35 [00:00<00:00, 174.91batch/s, loss=0.898]\n",
      "Epoch 64/200: 100%|██████████| 35/35 [00:00<00:00, 176.51batch/s, loss=0.719]\n",
      "Epoch 65/200: 100%|██████████| 35/35 [00:00<00:00, 169.92batch/s, loss=0.806]\n",
      "Epoch 66/200: 100%|██████████| 35/35 [00:00<00:00, 175.19batch/s, loss=0.945]\n",
      "Epoch 67/200: 100%|██████████| 35/35 [00:00<00:00, 174.95batch/s, loss=0.691]\n",
      "Epoch 68/200: 100%|██████████| 35/35 [00:00<00:00, 169.59batch/s, loss=0.874]\n",
      "Epoch 69/200: 100%|██████████| 35/35 [00:00<00:00, 172.16batch/s, loss=0.777]\n",
      "Epoch 70/200: 100%|██████████| 35/35 [00:00<00:00, 166.78batch/s, loss=0.849]\n",
      "Epoch 71/200: 100%|██████████| 35/35 [00:00<00:00, 172.78batch/s, loss=0.693]\n",
      "Epoch 72/200: 100%|██████████| 35/35 [00:00<00:00, 174.06batch/s, loss=1.08] \n",
      "Epoch 73/200: 100%|██████████| 35/35 [00:00<00:00, 174.23batch/s, loss=0.698]\n",
      "Epoch 74/200: 100%|██████████| 35/35 [00:00<00:00, 175.80batch/s, loss=0.834]\n",
      "Epoch 75/200: 100%|██████████| 35/35 [00:00<00:00, 174.93batch/s, loss=0.876]\n",
      "Epoch 76/200: 100%|██████████| 35/35 [00:00<00:00, 168.56batch/s, loss=0.699]\n",
      "Epoch 77/200: 100%|██████████| 35/35 [00:00<00:00, 174.06batch/s, loss=0.893]\n",
      "Epoch 78/200: 100%|██████████| 35/35 [00:00<00:00, 172.57batch/s, loss=1.07] \n",
      "Epoch 79/200: 100%|██████████| 35/35 [00:00<00:00, 173.76batch/s, loss=0.844]\n",
      "Epoch 80/200: 100%|██████████| 35/35 [00:00<00:00, 166.17batch/s, loss=0.752]\n",
      "Epoch 81/200: 100%|██████████| 35/35 [00:00<00:00, 166.32batch/s, loss=0.703]\n",
      "Epoch 82/200: 100%|██████████| 35/35 [00:00<00:00, 185.65batch/s, loss=0.828]\n",
      "Epoch 83/200: 100%|██████████| 35/35 [00:00<00:00, 167.07batch/s, loss=0.73] \n",
      "Epoch 84/200: 100%|██████████| 35/35 [00:00<00:00, 185.55batch/s, loss=0.884]\n",
      "Epoch 85/200: 100%|██████████| 35/35 [00:00<00:00, 164.56batch/s, loss=0.721]\n",
      "Epoch 86/200: 100%|██████████| 35/35 [00:00<00:00, 155.05batch/s, loss=0.862]\n",
      "Epoch 87/200: 100%|██████████| 35/35 [00:00<00:00, 186.08batch/s, loss=0.731]\n",
      "Epoch 88/200: 100%|██████████| 35/35 [00:00<00:00, 172.09batch/s, loss=0.873]\n",
      "Epoch 89/200: 100%|██████████| 35/35 [00:00<00:00, 173.67batch/s, loss=0.701]\n",
      "Epoch 90/200: 100%|██████████| 35/35 [00:00<00:00, 176.70batch/s, loss=1.41] \n",
      "Epoch 91/200: 100%|██████████| 35/35 [00:00<00:00, 175.02batch/s, loss=0.698]\n",
      "Epoch 92/200: 100%|██████████| 35/35 [00:00<00:00, 161.19batch/s, loss=1.07] \n",
      "Epoch 93/200: 100%|██████████| 35/35 [00:00<00:00, 172.99batch/s, loss=0.686]\n",
      "Epoch 94/200: 100%|██████████| 35/35 [00:00<00:00, 165.69batch/s, loss=0.872]\n",
      "Epoch 95/200: 100%|██████████| 35/35 [00:00<00:00, 176.45batch/s, loss=0.703]\n",
      "Epoch 96/200: 100%|██████████| 35/35 [00:00<00:00, 183.50batch/s, loss=0.665]\n",
      "Epoch 97/200: 100%|██████████| 35/35 [00:00<00:00, 189.05batch/s, loss=0.687]\n",
      "Epoch 98/200: 100%|██████████| 35/35 [00:00<00:00, 186.20batch/s, loss=0.713]\n",
      "Epoch 99/200: 100%|██████████| 35/35 [00:00<00:00, 183.41batch/s, loss=0.695]\n",
      "Epoch 100/200: 100%|██████████| 35/35 [00:00<00:00, 178.82batch/s, loss=0.707]\n",
      "Epoch 101/200: 100%|██████████| 35/35 [00:00<00:00, 183.01batch/s, loss=1.25] \n",
      "Epoch 102/200: 100%|██████████| 35/35 [00:00<00:00, 188.69batch/s, loss=0.768]\n",
      "Epoch 103/200: 100%|██████████| 35/35 [00:00<00:00, 185.35batch/s, loss=1.28] \n",
      "Epoch 104/200: 100%|██████████| 35/35 [00:00<00:00, 188.35batch/s, loss=0.686]\n",
      "Epoch 105/200: 100%|██████████| 35/35 [00:00<00:00, 190.85batch/s, loss=1.52] \n",
      "Epoch 106/200: 100%|██████████| 35/35 [00:00<00:00, 180.69batch/s, loss=0.69] \n",
      "Epoch 107/200: 100%|██████████| 35/35 [00:00<00:00, 183.86batch/s, loss=0.626]\n",
      "Epoch 108/200: 100%|██████████| 35/35 [00:00<00:00, 171.92batch/s, loss=0.94] \n",
      "Epoch 109/200: 100%|██████████| 35/35 [00:00<00:00, 166.70batch/s, loss=0.626]\n",
      "Epoch 110/200: 100%|██████████| 35/35 [00:00<00:00, 183.89batch/s, loss=0.663]\n",
      "Epoch 111/200: 100%|██████████| 35/35 [00:00<00:00, 180.99batch/s, loss=0.624]\n",
      "Epoch 112/200: 100%|██████████| 35/35 [00:00<00:00, 178.20batch/s, loss=0.686]\n",
      "Epoch 113/200: 100%|██████████| 35/35 [00:00<00:00, 166.67batch/s, loss=0.702]\n",
      "Epoch 114/200: 100%|██████████| 35/35 [00:00<00:00, 176.03batch/s, loss=0.625]\n",
      "Epoch 115/200: 100%|██████████| 35/35 [00:00<00:00, 177.80batch/s, loss=0.708]\n",
      "Epoch 116/200: 100%|██████████| 35/35 [00:00<00:00, 171.18batch/s, loss=0.623]\n",
      "Epoch 117/200: 100%|██████████| 35/35 [00:00<00:00, 179.00batch/s, loss=0.687]\n",
      "Epoch 118/200: 100%|██████████| 35/35 [00:00<00:00, 177.16batch/s, loss=0.845]\n",
      "Epoch 119/200: 100%|██████████| 35/35 [00:00<00:00, 177.50batch/s, loss=0.625]\n",
      "Epoch 120/200: 100%|██████████| 35/35 [00:00<00:00, 176.52batch/s, loss=0.797]\n",
      "Epoch 121/200: 100%|██████████| 35/35 [00:00<00:00, 175.57batch/s, loss=0.662]\n",
      "Epoch 122/200: 100%|██████████| 35/35 [00:00<00:00, 177.54batch/s, loss=0.708]\n",
      "Epoch 123/200: 100%|██████████| 35/35 [00:00<00:00, 175.87batch/s, loss=0.72] \n",
      "Epoch 124/200: 100%|██████████| 35/35 [00:00<00:00, 168.58batch/s, loss=0.621]\n",
      "Epoch 125/200: 100%|██████████| 35/35 [00:00<00:00, 172.91batch/s, loss=1.07] \n",
      "Epoch 126/200: 100%|██████████| 35/35 [00:00<00:00, 169.49batch/s, loss=0.751]\n",
      "Epoch 127/200: 100%|██████████| 35/35 [00:00<00:00, 178.45batch/s, loss=0.704]\n",
      "Epoch 128/200: 100%|██████████| 35/35 [00:00<00:00, 176.76batch/s, loss=1.19] \n",
      "Epoch 129/200: 100%|██████████| 35/35 [00:00<00:00, 179.63batch/s, loss=0.626]\n",
      "Epoch 130/200: 100%|██████████| 35/35 [00:00<00:00, 169.60batch/s, loss=0.72] \n",
      "Epoch 131/200: 100%|██████████| 35/35 [00:00<00:00, 173.60batch/s, loss=0.62] \n",
      "Epoch 132/200: 100%|██████████| 35/35 [00:00<00:00, 180.10batch/s, loss=0.67] \n",
      "Epoch 133/200: 100%|██████████| 35/35 [00:00<00:00, 175.12batch/s, loss=0.703]\n",
      "Epoch 134/200: 100%|██████████| 35/35 [00:00<00:00, 172.87batch/s, loss=0.728]\n",
      "Epoch 135/200: 100%|██████████| 35/35 [00:00<00:00, 176.82batch/s, loss=1.19] \n",
      "Epoch 136/200: 100%|██████████| 35/35 [00:00<00:00, 167.80batch/s, loss=0.713]\n",
      "Epoch 137/200: 100%|██████████| 35/35 [00:00<00:00, 170.62batch/s, loss=0.859]\n",
      "Epoch 138/200: 100%|██████████| 35/35 [00:00<00:00, 171.72batch/s, loss=0.721]\n",
      "Epoch 139/200: 100%|██████████| 35/35 [00:00<00:00, 180.17batch/s, loss=0.72] \n",
      "Epoch 140/200: 100%|██████████| 35/35 [00:00<00:00, 177.30batch/s, loss=1.07] \n",
      "Epoch 141/200: 100%|██████████| 35/35 [00:00<00:00, 165.29batch/s, loss=0.952]\n",
      "Epoch 142/200: 100%|██████████| 35/35 [00:00<00:00, 180.24batch/s, loss=0.915]\n",
      "Epoch 143/200: 100%|██████████| 35/35 [00:00<00:00, 177.86batch/s, loss=0.674]\n",
      "Epoch 144/200: 100%|██████████| 35/35 [00:00<00:00, 179.32batch/s, loss=1.28] \n",
      "Epoch 145/200: 100%|██████████| 35/35 [00:00<00:00, 180.63batch/s, loss=0.738]\n",
      "Epoch 146/200: 100%|██████████| 35/35 [00:00<00:00, 151.77batch/s, loss=0.764]\n",
      "Epoch 147/200: 100%|██████████| 35/35 [00:00<00:00, 168.98batch/s, loss=0.938]\n",
      "Epoch 148/200: 100%|██████████| 35/35 [00:00<00:00, 177.14batch/s, loss=0.789]\n",
      "Epoch 149/200: 100%|██████████| 35/35 [00:00<00:00, 172.78batch/s, loss=0.633]\n",
      "Epoch 150/200: 100%|██████████| 35/35 [00:00<00:00, 178.01batch/s, loss=0.804]\n",
      "Epoch 151/200: 100%|██████████| 35/35 [00:00<00:00, 179.84batch/s, loss=0.726]\n",
      "Epoch 152/200: 100%|██████████| 35/35 [00:00<00:00, 181.48batch/s, loss=0.731]\n",
      "Epoch 153/200: 100%|██████████| 35/35 [00:00<00:00, 179.98batch/s, loss=0.686]\n",
      "Epoch 154/200: 100%|██████████| 35/35 [00:00<00:00, 180.74batch/s, loss=0.85] \n",
      "Epoch 155/200: 100%|██████████| 35/35 [00:00<00:00, 176.41batch/s, loss=0.668]\n",
      "Epoch 156/200: 100%|██████████| 35/35 [00:00<00:00, 178.42batch/s, loss=0.899]\n",
      "Epoch 157/200: 100%|██████████| 35/35 [00:00<00:00, 180.40batch/s, loss=0.653]\n",
      "Epoch 158/200: 100%|██████████| 35/35 [00:00<00:00, 178.01batch/s, loss=0.611]\n",
      "Epoch 159/200: 100%|██████████| 35/35 [00:00<00:00, 177.79batch/s, loss=1.06] \n",
      "Epoch 160/200: 100%|██████████| 35/35 [00:00<00:00, 175.00batch/s, loss=0.716]\n",
      "Epoch 161/200: 100%|██████████| 35/35 [00:00<00:00, 175.99batch/s, loss=0.726]\n",
      "Epoch 162/200: 100%|██████████| 35/35 [00:00<00:00, 181.05batch/s, loss=0.774]\n",
      "Epoch 163/200: 100%|██████████| 35/35 [00:00<00:00, 179.82batch/s, loss=0.927]\n",
      "Epoch 164/200: 100%|██████████| 35/35 [00:00<00:00, 181.38batch/s, loss=0.811]\n",
      "Epoch 165/200: 100%|██████████| 35/35 [00:00<00:00, 175.30batch/s, loss=0.68] \n",
      "Epoch 166/200: 100%|██████████| 35/35 [00:00<00:00, 171.56batch/s, loss=0.687]\n",
      "Epoch 167/200: 100%|██████████| 35/35 [00:00<00:00, 180.18batch/s, loss=0.637]\n",
      "Epoch 168/200: 100%|██████████| 35/35 [00:00<00:00, 176.98batch/s, loss=0.622]\n",
      "Epoch 169/200: 100%|██████████| 35/35 [00:00<00:00, 179.17batch/s, loss=0.787]\n",
      "Epoch 170/200: 100%|██████████| 35/35 [00:00<00:00, 175.19batch/s, loss=0.686]\n",
      "Epoch 171/200: 100%|██████████| 35/35 [00:00<00:00, 170.45batch/s, loss=1.04] \n",
      "Epoch 172/200: 100%|██████████| 35/35 [00:00<00:00, 180.26batch/s, loss=0.894]\n",
      "Epoch 173/200: 100%|██████████| 35/35 [00:00<00:00, 179.16batch/s, loss=1.17] \n",
      "Epoch 174/200: 100%|██████████| 35/35 [00:00<00:00, 180.52batch/s, loss=0.851]\n",
      "Epoch 175/200: 100%|██████████| 35/35 [00:00<00:00, 179.63batch/s, loss=0.719]\n",
      "Epoch 176/200: 100%|██████████| 35/35 [00:00<00:00, 178.63batch/s, loss=0.883]\n",
      "Epoch 177/200: 100%|██████████| 35/35 [00:00<00:00, 176.63batch/s, loss=0.562]\n",
      "Epoch 178/200: 100%|██████████| 35/35 [00:00<00:00, 180.21batch/s, loss=0.624]\n",
      "Epoch 179/200: 100%|██████████| 35/35 [00:00<00:00, 174.96batch/s, loss=1.73] \n",
      "Epoch 180/200: 100%|██████████| 35/35 [00:00<00:00, 177.50batch/s, loss=0.801]\n",
      "Epoch 181/200: 100%|██████████| 35/35 [00:00<00:00, 178.26batch/s, loss=0.702]\n",
      "Epoch 182/200: 100%|██████████| 35/35 [00:00<00:00, 178.85batch/s, loss=0.756]\n",
      "Epoch 183/200: 100%|██████████| 35/35 [00:00<00:00, 175.18batch/s, loss=1.01] \n",
      "Epoch 184/200: 100%|██████████| 35/35 [00:00<00:00, 177.87batch/s, loss=0.757]\n",
      "Epoch 185/200: 100%|██████████| 35/35 [00:00<00:00, 179.86batch/s, loss=0.906]\n",
      "Epoch 186/200: 100%|██████████| 35/35 [00:00<00:00, 175.98batch/s, loss=1.2]  \n",
      "Epoch 187/200: 100%|██████████| 35/35 [00:00<00:00, 181.59batch/s, loss=0.721]\n",
      "Epoch 188/200: 100%|██████████| 35/35 [00:00<00:00, 180.27batch/s, loss=0.623]\n",
      "Epoch 189/200: 100%|██████████| 35/35 [00:00<00:00, 176.78batch/s, loss=0.712]\n",
      "Epoch 190/200: 100%|██████████| 35/35 [00:00<00:00, 179.96batch/s, loss=0.686]\n",
      "Epoch 191/200: 100%|██████████| 35/35 [00:00<00:00, 174.27batch/s, loss=0.718]\n",
      "Epoch 192/200: 100%|██████████| 35/35 [00:00<00:00, 177.61batch/s, loss=0.483]\n",
      "Epoch 193/200: 100%|██████████| 35/35 [00:00<00:00, 181.20batch/s, loss=0.693]\n",
      "Epoch 194/200: 100%|██████████| 35/35 [00:00<00:00, 177.66batch/s, loss=0.916]\n",
      "Epoch 195/200: 100%|██████████| 35/35 [00:00<00:00, 176.59batch/s, loss=0.528]\n",
      "Epoch 196/200: 100%|██████████| 35/35 [00:00<00:00, 176.90batch/s, loss=1.15] \n",
      "Epoch 197/200: 100%|██████████| 35/35 [00:00<00:00, 176.30batch/s, loss=0.764]\n",
      "Epoch 198/200: 100%|██████████| 35/35 [00:00<00:00, 179.80batch/s, loss=0.793]\n",
      "Epoch 199/200: 100%|██████████| 35/35 [00:00<00:00, 179.39batch/s, loss=0.792]\n",
      "Epoch 200/200: 100%|██████████| 35/35 [00:00<00:00, 168.06batch/s, loss=0.698]\n",
      "Evaluating: 100%|██████████| 9/9 [00:00<00:00, 11.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5143\n",
      "Model weights saved to model_weights.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:49:14.773918Z",
     "start_time": "2024-12-19T14:49:14.772736Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e91c6a72-07b9-42ee-bc4d-ad55ebf399e7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
